# AI-Assisted Development Playbook

[![Read in Russian](https://img.shields.io/badge/Lang-Ð ÑƒÑÑÐºÐ¸Ð¹-blue)](README.md)
[![ç¹é«”ä¸­æ–‡](https://img.shields.io/badge/Lang-ç¹é«”ä¸­æ–‡-blue)](README.zh.md)

> **Note:** This translation was performed by Gemini CLI (gemini-3-flash-preview).

This repository contains the configuration and rules used for the AI-assisted development of the [TasK](https://task.ai-aid.pro/) project. It serves as a public example of organizing documentation and workflows for AI agents (within Gemini CLI, Codex CLI, Kilo Code, or similar environments). Here you will find rules, role instructions, and templates that enable effective development management using LLMs.

I am publishing these materials as an example of a real-world workflow to share my experience, discuss AI development approaches, and find ways to improve them together. You are free to study, adapt, and apply these practices in your own projects.

## Workflow

I have arrived at an approach I call **Task-driven development** â€” development driven by tasks as specifications.

In this approach, the unit of truth is not a "general requirement description," but a specific task (or epic) formatted according to a strict template. The task acts as a specification for execution: it defines the goal, boundaries (scope / out of scope), acceptance criteria, and mandatory checks. If necessary, it includes requirements for tests (unit, integration, e2e). Implementation is considered ready only after confirming compliance with the task: passing checks, executing tests, and a final review. Otherwise, the task is refined, and the cycle repeats.

**Difference from Spec-driven development.** Spec-driven development is built around a separate specification artifact (API contract, behavioral scenarios, formal model) against which the implementation is written. In task-driven development, the specification is "packaged" directly into the task: the task = the spec. Task setting becomes the central element of the process, and development becomes the process of proving that the code satisfies the task's formulations.

## ðŸ§  Core Manifesto (AGENTS.md)

The [AGENTS.md](./AGENTS.en.md) file is the entry point and "constitution" for the AI agent. It contains the following sections:
* **Mission** and rule priority.
* **Role** â€” selecting a specialized role before starting work.
* **Reflection** â€” assessing task complexity, context, and risks.
* **Language** â€” communication and naming rules.
* **Project Architecture** â€” stack, infrastructure, folder structure, migrations, modules, and layers.
* **Working with Code** â€” Git-flow, branches, task management, and technical debt.
* **Tests and Validation** â€” test types, tools, and `make check`.
* **Pre-checks** â€” requirements before submitting a task.
* **Pull Requests** and **Commit Format**.
* **Documentation** and **Prohibitions**.
* **Mini-checklist (for self-check)**.

## ðŸŽ­ Agent Roles

Depending on the task, the agent assumes one of the specialized roles. Role descriptions are located in `docs/agents/roles/team/` (files are in Russian):

* **[Product Owner (PO)](docs/agents/roles/team/product_owner.en.md)** â€” product management.
* **[Analyst](docs/agents/roles/team/system_analyst.en.md)** â€” requirements analysis and decomposition.
* **[Architect](docs/agents/roles/team/system_architect.en.md)** â€” system design and integrity control.
* **[Lead](docs/agents/roles/team/team_lead.en.md)** â€” coordination and decision making.
* **[Backend Developer](docs/agents/roles/team/backend_developer.en.md)** â€” server-side development.
* **[UI/UX Designer](docs/agents/roles/team/ui_ux_designer.en.md)** â€” user experience and interface design.
* **[Frontend Developer](docs/agents/roles/team/frontend_developer.en.md)** â€” client-side development.
* **[DevOps](docs/agents/roles/team/devops_engineer.en.md)** â€” infrastructure and CI/CD.
* **[Backend Reviewer](docs/agents/roles/team/code_reviewer_backend.en.md)** â€” code quality check.
* **[Frontend Reviewer](docs/agents/roles/team/code_reviewer_frontend.en.md)** â€” UI/UX and code quality check.
* **[DevOps Reviewer](docs/agents/roles/team/code_reviewer_devops.en.md)** â€” infrastructure and security review.
* **[Backend QA](docs/agents/roles/team/qa_backend.en.md)** â€” server-side testing.
* **[Frontend QA](docs/agents/roles/team/qa_frontend.en.md)** â€” client-side testing.
* **[Technical Writer](docs/agents/roles/team/technical_writer.en.md)** â€” user documentation and help.
* **[Copywriter](docs/agents/roles/team/copywriter.en.md)** â€” content marketing and storytelling.

**Examples of addressing roles in a request:**
* `Backend Developer take the task from todo/EPIC-status-page.todo.md to work`
* `DevOps check the changes in devops/nginx/conf.d/dev/task.conf, do we need everything there? Are we overcomplicating it?`
* `Frontend Developer review the file apps/web/assets/controllers/notification-toast_controller.js`

**Examples of addressing roles in a request:**
* `Backend Developer take the task from todo/EPIC-status-page.todo.md to work`
* `DevOps check the changes in devops/nginx/conf.d/dev/task.conf, do we need everything there? Are we overcomplicating it?`
* `Frontend Developer review the file apps/web/assets/controllers/notification-toast_controller.js`

## ðŸ“ Task Management (Todo)

A file-based task management system in the [`todo/`](./todo/) directory is used for setting tasks. This allows the agent to receive tasks as part of the project context.

* **[Task Rules](./todo/AGENTS.md)** â€” instructions on the task life cycle (creation, execution, completion).
* **[Task Template](./todo/templates/task.md)** â€” file structure for a single task.
* **[Epic Template](./todo/templates/epic.md)** â€” structure for large features and stories.

## ðŸš€ How It Works

Everything is built on `AGENTS.md` â€” a file with project rules and conventions. Processes, templates, and transition rules are described there, ensuring the agent works predictably and results are repeatable.

The processes and documents are not final â€” I am constantly improving them. The goals are to increase the quality of the agent's solutions and its autonomy. The more I trust the agent, the less I participate in development.

I no longer write code by hand â€” only minor edits and markdown documents. But my participation is still significant: I cannot trust models 100%, and I have to verify. The agent breaks project rules, layer isolation, namespace and class naming, and writes redundant tests.

### Task Setting Process

1. **Request.** I make a request to the AI agent. Example: `Take on the role of an analyst. I need a status page for the project, create an epic for this task.`
2. **Generation.** The agent loads the role, task setting requirements, and templates â€” and generates the task text.
3. **Self-review.** I ask the agent to check itself. If a specific direction (architect, devops, frontend/backend developer) needs work, I ask it to take the corresponding role.
4. **Refinement.** If there are comments, I ask for corrections and we return to step 2.
5. **PR Creation.** If everything is fine, I ask the agent to create a PR.
6. **Final Review.** I check the task setting myself, going through "comment â€” correction" iterations with the agent.
7. **Closing.** I merge the PR and inform the agent. It deletes the branch, switches to master, selects the next task, and proposes to start.

```mermaid
flowchart LR
    A[Request] --> B[Generation]
    B --> C[Self review]
    C --> D{Any comments?}
    D -->|Yes| B
    D -->|No| E[Create PR]
    E --> F[Final Review]
    F --> G{Comments?}
    G -->|Yes| F
    G -->|No| H[Close]
```

### Task Implementation Process

The process is similar to task setting, but the agent performs more checks independently before showing me the code.

1. **Request.** Example: `You are a [Backend Developer](docs/agents/roles/team/backend_developer.en.md). Take the task todo/EPIC-status-page.todo.md to work.`
2. **Implementation.** The agent fulfills the task requirements and runs checks itself: tests (PHPUnit), static analysis (PHPMD, Deptrac, Psalm), style validation (PHP_CodeSniffer), build (Composer). This creates a self-validation cycle â€” the agent delivers code that is already clean enough.
3. **Self-review.** I ask the agent to check the solution. I can ask it to take a role (architect, devops, frontend/backend developer) and run checks sequentially.
4. **Refinement.** If there are comments, I ask for corrections and we return to step 2.
5. **PR Creation.** If everything is fine, I ask the agent to create a PR.
6. **Final Review.** I check the code myself, going through "comment â€” correction" iterations with the agent.
7. **Closing.** I merge the PR and inform the agent. It deletes the branch, switches to master, selects the next task.
8. **Accumulation.** Tasks accumulate for the release.
9. **Release Preparation.** I ask the agent to run e2e tests and prepare the release: tag, changelog, publication on GitHub.
10. **Release.** I deploy to prod: configurations, dependencies, migrations, supervisor restart. Then â€” post-checks.

```mermaid
flowchart LR
    A[Request] --> B[Implementation]
    B --> C[Self review]
    C --> D{Any comments?}
    D -->|Yes| B
    D -->|No| E[Create PR]
    E --> F[Final Review]
    F --> G{Comments?}
    G -->|Yes| F
    G -->|No| H[Close]
    H --> I[Accumulation]
    I --> J{Release?}
    J -->|No| A
    J -->|Yes| K[Prep Release]
    K --> L[Release]
```

### Continuous Improvement Process (Retrospective)

This process aims to continuously increase the agent's autonomy and work quality. It closes the development loop by transforming identified problems into updated standards and automated checks.

1. **Observation.** Monitor the agent's work in real-time. Record any hesitations, context misunderstandings, or errors that surface during the review stage.
2. **Analysis.** Identify recurring error patterns that waste resources (time, tokens). Seek a systemic solution: how to modify instructions or tools to prevent the error from recurring.
3. **Improvement.** Apply targeted edits to `AGENTS.md`, task templates, or linter configs. Update the project knowledge base.

```mermaid
flowchart LR
    A[Observation] --> B[Analysis]
    B --> C[Improvement]
    C --> A
```

> **Important:** Adhere to the principle of isolated changes. Do not change everything at once â€” this makes it impossible to track the impact of a specific edit. Implement improvements in small batches and verify the effect immediately.

## ðŸ“‚ Implementation Examples

To better understand how these rules work in practice, you can examine real artifacts created by AI agents:

*   **[Epic Example](./todo/EPIC-status-page.todo.md)** â€” a full specification for a major feature (Status Page), created by an agent in the Analyst role.
*   **Tasks** â€” the [`todo/`](./todo/) and [`todo/done/`](./todo/done/) directories contain specific task files into which this epic was decomposed.
*   **Code Examples** â€” implementation logic written by an agent based on these tasks:
    *   [Core](./src/Module/Health) â€” logic, services, and integrations.
    *   [Web](./apps/web/src/Module/Health) â€” controllers and page templates.
*   **Test Examples** â€” tests created by the agent to verify the implementation:
    *   [Core Tests](./tests) â€” unit and integration tests.
    *   [Web Tests](./apps/web/tests) â€” unit and e2e tests.

### ðŸ“¸ Example with Screenshots

[In my blog](https://prikotov.pro/blog/pervyi-opyt-s-glm-5-koding-cherez-kilo-code#primer-raboty-v-kilo-code) â€” a detailed walkthrough of a real AI agent session with screenshots: from request to finished PR. Shows how the agent works with this playbook in practice.
