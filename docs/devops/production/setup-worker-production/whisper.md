# Установка whisper.cpp для распознавания речи


## Пути и права

- Утилиты располагаются в каталоге `/opt/whisper.cpp`. В примерах используется последняя версия;
  для обновлений просто обновите репозиторий.
## Системные зависимости

### Базовые пакеты

Установите необходимые пакеты для сборки и работы утилит (инструкция для AlmaLinux 10+):

> **Примечание:** Основные зависимости уже описаны в основном руководстве по настройке worker.
> Убедитесь, что установлены следующие пакеты: `gcc gcc-c++ make cmake git python3 python3-pip python3-virtualenv ffmpeg`.

### Поддержка NVIDIA GPU (опционально)

Для ускорения распознавания речи с использованием NVIDIA GPU необходимо предварительно установить драйверы и CUDA Toolkit.

> **Важно:** Перед установкой whisper.cpp выполните инструкции из документа: [Установка NVIDIA-драйверов](nvidia.md)

После правильной установки драйверов whisper.cpp сможет использовать GPU для ускорения обработки аудио.

## Сборка whisper.cpp

### Клонирование и сборка

Выполните следующие команды для клонирования репозитория и сборки последней версии:

```bash
# Клонируем репозиторий
git clone https://github.com/ggerganov/whisper.cpp.git /opt/whisper.cpp
cd /opt/whisper.cpp

# Определяем и устанавливаем последнюю стабильную версию
WHISPER_CPP_VERSION=$(git describe --tags --abbrev=0)
echo "Installing whisper.cpp version: ${WHISPER_CPP_VERSION}"
git checkout ${WHISPER_CPP_VERSION}

# Собираем с поддержкой NVIDIA GPU (если драйверы установлены)
cmake -B build -DGGML_CUDA=1
cmake --build build -j --config Release
```

> **Примечание:** Если NVIDIA драйверы не установлены, выполните сборку без поддержки GPU:
> ```bash
> cmake -B build
> cmake --build build -j --config Release
> ```



## Модели распознавания

### Загрузка модели

Whisper.cpp использует модели нейросети для распознавания речи. Рекомендуемая модель для production — `ggml-large-v3.bin`.

```bash
cd /opt/whisper.cpp
# Скачиваем рекомендуемую модель
./models/download-ggml-model.sh large-v3
```

> **Доступные модели:** tiny, base, small, medium, large-v1, large-v2, large-v3
> **Рекомендация:** Для качественного распознавания русской речи используйте `large-v3`

### Создание символических ссылок (опционально)

Для удобного доступа к утилитам можно создать символические ссылки:

```bash
# Раскомментируйте при необходимости
#ln -sf /opt/whisper.cpp/build/bin/whisper-cli /usr/local/bin/whisper-cli
#ln -sf /opt/whisper.cpp/build/bin/whisper-server /usr/local/bin/whisper-server
```

> **Примечание:** Если вы используете другую модель, обязательно обновите переменную окружения `WHISPER_CPP_ASR_MODEL` в конфигурации.

## Конфигурация

### Переменные окружения

Добавьте необходимые переменные в `.env.local` (для разработки) или в Secrets (для production):

```dotenv
# Обязательные переменные
WHISPER_CPP_UTILS_PATH=/opt/whisper.cpp
WHISPER_CPP_ASR_MODEL=ggml-large-v3.bin
```

> **Важно:** `WHISPER_CPP_UTILS_PATH` должен указывать на директорию `/opt/whisper.cpp`.

Система также использует множество дополнительных переменных окружения для тонкой настройки производительности, параметров распознавания и обработки аудио. Все они имеют значения по умолчанию и могут быть переопределены при необходимости. Полный список переменных доступен в документации компонента.

## Проверка работоспособности

### Проверка бинарных файлов

Убедитесь, что утилиты whisper доступны и исполняются:

```bash
whisper-cli --help
whisper-server --help
```

### Тестовый запуск сервера

Выполните быстрый тест сервера распознавания:

```bash
# Запускаем сервер в фоновом режиме
whisper-server \
  --model /opt/whisper.cpp/models/ggml-large-v3.bin \
  --language ru --threads 8 --port 8080 &

# Ждём запуска и проверяем доступность
sleep 5
curl http://127.0.0.1:8080/

# Останавливаем тестовый сервер
kill %1
```

### Завершение

Если все команды выполняются без ошибок, окружение готово к работе. Компоненты PHP смогут автоматически запускать `whisper-server`, отправлять на него аудиосегменты и получать результаты распознавания.
